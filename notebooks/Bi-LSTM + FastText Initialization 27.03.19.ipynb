{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/Cellar/python/3.7.2_2/Frameworks/Python.framework/Versions/3.7/lib/python3.7/importlib/_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 216, got 192\n",
      "  return f(*args, **kwds)\n",
      "/usr/local/Cellar/python/3.7.2_2/Frameworks/Python.framework/Versions/3.7/lib/python3.7/importlib/_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 216, got 192\n",
      "  return f(*args, **kwds)\n",
      "/usr/local/Cellar/python/3.7.2_2/Frameworks/Python.framework/Versions/3.7/lib/python3.7/importlib/_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 216, got 192\n",
      "  return f(*args, **kwds)\n",
      "/usr/local/Cellar/python/3.7.2_2/Frameworks/Python.framework/Versions/3.7/lib/python3.7/importlib/_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 216, got 192\n",
      "  return f(*args, **kwds)\n",
      "/usr/local/Cellar/python/3.7.2_2/Frameworks/Python.framework/Versions/3.7/lib/python3.7/importlib/_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject\n",
      "  return f(*args, **kwds)\n",
      "/usr/local/Cellar/python/3.7.2_2/Frameworks/Python.framework/Versions/3.7/lib/python3.7/importlib/_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject\n",
      "  return f(*args, **kwds)\n",
      "/usr/local/Cellar/python/3.7.2_2/Frameworks/Python.framework/Versions/3.7/lib/python3.7/importlib/_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 216, got 192\n",
      "  return f(*args, **kwds)\n",
      "/usr/local/Cellar/python/3.7.2_2/Frameworks/Python.framework/Versions/3.7/lib/python3.7/importlib/_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from src.preprocess.data_prep_offenseval import DataPrepOffensEval\n",
    "from src.classifiers.classifier_bi_lstm import BiLstmClassifier\n",
    "from src.utils.stats import get_distribution_from_y\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENGLISH_FILE_PATH = \"../data/raw/OffensEval2019/start-kit/training-v1/offenseval-training-v1.tsv\"\n",
    "def get_X_y():\n",
    "    dp = DataPrepOffensEval()\n",
    "    result_tuple = dp.get_X_and_ys(file_path=ENGLISH_FILE_PATH)\n",
    "    X_original = result_tuple[0]\n",
    "    y_sub_a = result_tuple[1]\n",
    "    return X_original, y_sub_a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Format our text samples and labels into tensors that can be fed into \n",
    "# a NN. To do this, we will rely on Keras utilities\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "MAX_NB_WORDS = 20000 # Max number of words in tweet to consider, ranked by frequency\n",
    "MAX_SEQ_LENGTH = 1000 # Lengt of tensors that are fed into NN\n",
    "\n",
    "def tokenize(X, max_nb_words=MAX_NB_WORDS, max_seq_length=MAX_SEQ_LENGTH):\n",
    "    tokenizer = Tokenizer(num_words=max_nb_words)\n",
    "    tokenizer.fit_on_texts(X)\n",
    "    sequences = tokenizer.texts_to_sequences(X)\n",
    "    word_index = tokenizer.word_index # Number of unique tokens\n",
    "    X = pad_sequences(sequences, maxlen=max_seq_length) # PAD Everything to be the same length - necessary for keras\n",
    "    return X, word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = get_X_y()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, word_index = tokenize(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dp = DataPrepOffensEval()\n",
    "X_train, X_test, y_train, y_test = dp.train_test_split(X, y, test_size=0.2)\n",
    "X_train, X_val, y_train, y_val = dp.train_test_split(X_train, y_train, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "NUM_VECTORS = 50000\n",
    "ENGLISH_EMB_FILE_PATH = \"../cc.en.300.vec\"\n",
    "\n",
    "en_model = KeyedVectors.load_word2vec_format(\n",
    "    ENGLISH_EMB_FILE_PATH,\n",
    "    limit=NUM_VECTORS,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def get_embedding_matrix(model=en_model, X=X, embedding_dim=300, word_index=word_index):\n",
    "    embedding_matrix = np.zeros((len(word_index) + 1, embedding_dim))\n",
    "    for word, i in word_index.items():\n",
    "        if word in model:\n",
    "            embedding_vector = model[word]\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "            # Words not found will be all zeros\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = get_embedding_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create the embedding layer from the embedding matrix for keras\n",
    "from keras.layers import Embedding\n",
    "EMBEDDING_DIM = 300\n",
    "\n",
    "embedding_layer = Embedding(\n",
    "    len(word_index) + 1,\n",
    "    EMBEDDING_DIM,\n",
    "    weights=[embedding_matrix],\n",
    "    input_length=MAX_SEQ_LENGTH,\n",
    "    trainable=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import (\n",
    "    Bidirectional, \n",
    "    Dropout, \n",
    "    Dense,\n",
    "    Activation, \n",
    "    LSTM,\n",
    ")\n",
    "\n",
    "from keras import Sequential\n",
    "\n",
    "def create_model(\n",
    "    embedding_layer,\n",
    "    dropout_amount=0.2,\n",
    "    lstm_units=10,\n",
    "    dense_1_units=4,\n",
    "    dense_2_units=1,\n",
    "    dense_1_activation=\"relu\",\n",
    "    dense_2_activation=\"sigmoid\",\n",
    "    optimizer=\"adam\",\n",
    "    loss_func=\"binary_crossentropy\",\n",
    "    metrics=[\"accuracy\"],\n",
    "):\n",
    "    model = Sequential()\n",
    "    model.add(embedding_layer)\n",
    "    model.add(Bidirectional(\n",
    "        layer=LSTM(\n",
    "            units=lstm_units,\n",
    "            dropout=dropout_amount,\n",
    "            recurrent_dropout=dropout_amount,\n",
    "        )\n",
    "    ))\n",
    "    model.add(Dropout(dropout_amount))\n",
    "    model.add(Dense(\n",
    "        units=dense_1_units,\n",
    "    ))\n",
    "    model.add(Activation(dense_1_activation))\n",
    "    model.add(Dropout(dropout_amount))\n",
    "    model.add(Dense(\n",
    "        units=dense_2_units,\n",
    "    ))\n",
    "    model.add(Activation(dense_2_activation))\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss=loss_func,\n",
    "        metrics=metrics,\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "def train_model(\n",
    "    model,\n",
    "    X_train=X_train,\n",
    "    y_train=y_train,\n",
    "    epochs=20,\n",
    "    batch_size=512,\n",
    "    validation_data=[X_val,y_val],\n",
    "):\n",
    "    history = model.fit(\n",
    "        X_train,\n",
    "        y_train,\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size,\n",
    "        validation_data=validation_data,\n",
    "        verbose=2,\n",
    "    )\n",
    "    return history\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def plot_train_val_loss(history):\n",
    "    plt.clf()\n",
    "    history_dict = history.history\n",
    "    train_loss = history_dict['loss']\n",
    "    val_loss = history_dict['val_loss']\n",
    "    epochs = range(1, len(train_loss) + 1)\n",
    "    plt.plot(epochs, train_loss, 'r', label='Training Loss')\n",
    "    plt.plot(epochs, val_loss, 'b', label='Validation Loss')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    \n",
    "    \n",
    "def plot_train_val_accuracy(history):\n",
    "    plt.clf()\n",
    "    history_dict = history.history\n",
    "    train_metric = history_dict['acc']\n",
    "    val_metric = history_dict['val_' + 'acc']\n",
    "    epochs = range(1, len(train_metric) + 1)\n",
    "    plt.plot(epochs, train_metric, 'r', label='Training {}'.format('acc'))\n",
    "    plt.plot(epochs, val_metric, 'b', label='Validation {}'.format('acc'))\n",
    "    plt.title('Training and Validation {}'.format('acc'))\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('{}'.format('acc'))\n",
    "    plt.legend()\n",
    "\n",
    "    \n",
    "def predict_y(model, X_test=X_test):\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred_binary = [0 if value[0] < 0.5 else 1 for value in y_pred]\n",
    "    return y_pred_binary\n",
    "\n",
    "\n",
    "# Using this since we have the confusion matrix stuff implemented here\n",
    "# Move that stuff into a unique module\n",
    "from src.classifiers.classifier_bi_lstm import BiLstmClassifier\n",
    "\n",
    "\n",
    "def plot_confusion_matrix(\n",
    "    y_test,\n",
    "    y_pred,\n",
    "    num_categories=2,\n",
    "    category_names=[\"NOT\", \"OFF\"],\n",
    "):\n",
    "    classifier = BiLstmClassifier(\n",
    "        embedding_input_dim=1,\n",
    "        logfile=None,\n",
    "        epochs=1,\n",
    "    )\n",
    "    confusion_df = classifier.confusion_matrix(\n",
    "        y_test, y_pred, num_categories, category_names,\n",
    "    )\n",
    "    classifier.plot_confusion_matrix(\n",
    "        confusion_df, file_path=None,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/gudbjartursigurbergsson/PythonEnv/thesis/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /Users/gudbjartursigurbergsson/PythonEnv/thesis/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    }
   ],
   "source": [
    "model = create_model(embedding_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/gudbjartursigurbergsson/PythonEnv/thesis/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 9532 samples, validate on 1060 samples\n",
      "Epoch 1/20\n",
      " - 100s - loss: 0.6784 - acc: 0.6536 - val_loss: 0.6597 - val_acc: 0.6783\n",
      "Epoch 2/20\n",
      " - 103s - loss: 0.6523 - acc: 0.6615 - val_loss: 0.6314 - val_acc: 0.6783\n",
      "Epoch 3/20\n",
      " - 102s - loss: 0.6386 - acc: 0.6625 - val_loss: 0.6241 - val_acc: 0.6783\n",
      "Epoch 4/20\n",
      " - 103s - loss: 0.6351 - acc: 0.6626 - val_loss: 0.6198 - val_acc: 0.6783\n",
      "Epoch 5/20\n",
      " - 110s - loss: 0.6291 - acc: 0.6626 - val_loss: 0.6113 - val_acc: 0.6783\n",
      "Epoch 6/20\n"
     ]
    }
   ],
   "source": [
    "history = train_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "thesis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
